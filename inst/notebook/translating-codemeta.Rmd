---
title: "Translating between schema using JSON-LD"
author: "Carl Boettiger"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Translating between various software metaata formats with JSON-LD in codemetar}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---



```{r include=FALSE}
knitr::opts_chunk$set(comment="")
if(grepl("windows", tolower(Sys.info()[["sysname"]])))
  knitr::opts_chunk$set(comment="", error =TRUE)
```

```{r message=FALSE}
library("codemetar")
library("magrittr")
library("jsonlite")
library("jsonld")
```

## JSON-LD transforms: Expansion and Compaction

One of the central motivations of JSON-LD is making it easy to translate between different representations of what are fundamentally the same data types. Doing so uses the two core algorithms of JSON-LD: *expansion* and *compaction*, as [this excellent short video by JSON-LD creator Manu Sporny](https://www.youtube.com/watch?v=Tm3fD89dqRE) describes.

Here's how we would use JSON-LD (from R) to translate between the two examples of JSON data from different providers as shown in the video.  First, the JSON from the original provider:

```{r}
ex <-
'{
"@context":{
  "shouter": "http://schema.org/name",
  "txt": "http://schema.org/commentText"
},
"shouter": "Jim",
"txt": "Hello World!"
}'
```

Next, we need the context of the second data provider.  This will let us translate the JSON format used by provider one ("Shouttr") to the second ("BigHash"):

```{r}
bighash_context <- 
'{
"@context":{
  "user": "http://schema.org/name",
  "comment": "http://schema.org/commentText"
}
}'
```

With this in place, we simply expand the original JSON and then compact using the new context:

```{r}
jsonld_expand(ex) %>%
  jsonld_compact(context = bighash_context)
```

## Crosswalking

The CodeMeta Crosswalk table seeks to accomplish a very similar goal.  The crosswalk table provides a human-readable mapping of different software metadata providers into the codemeta context (an extension of schema.org).  For instance, we'll read in some data from GitHub:


### GitHub

Here we crosswalk the JSON data returned as the repository information from the GitHub API: 

```{r eval = FALSE}
repo_info <- gh::gh("/repos/:owner/:repo", owner = "ropensci", repo = "EML")
```

```{r include = FALSE}
## Actually, we'll use a chached copy and not eval the above chunk to avoid a `gh` dependency:
repo_info <- read_json(system.file("examples/github_format.json", package = "codemetar"))
```

Let's just take a look at what the returned json data looks like:

```{r}
repo_info %>% toJSON()
```

We can crosswalk this information into codemeta just by supplying the column name to the `crosswalk` function.  This performs the same expansion of the metadata in the GitHub context, followed by compaction into the codemeta context.  Note that terms not recognized/included in the codemeta context will be dropped:

```{r}
github_meta <- crosswalk(repo_info, "GitHub")
github_meta
```

We can verify that the result is a valid codemeta document:

```{r}
codemeta_validate(github_meta)
```


## Transforming into other column schema


The above transform showed the process of going from plain JSON data into the codemeta standard serialization.  Similarly, we can crosswalk into any of the other columns in the crosswalk table.  To do so, the `crosswalk` function will first expand any of the recognized properties into the codemeta JSON-LD context, just as above.  Unrecognized properties are dropped, since there is no consensus context into which we can expand them.  Second, the expanded terms are then compacted down into the new context (Zenodo in this case.)  This time,
any terms that are not part of the codemeta context are kept, but not compacted, since they still have meaningful contexts (that is, full [URI](https://en.wikipedia.org/wiki/Uniform_Resource_Identifier)s, e.g. URLs) that can be associated with them:

```{r}
crosswalk(repo_info, "GitHub", "Zenodo") %>%
drop_context()
```

Thus terms that still have a uncompacted prefix like `schema:` or `codemeta:` reflect properties that we could successfully extract from the input data, but do not have corresponding properties in the Zenodo context.  This is the standard behavior of the 
compaction algorithm: unrecognized fields are not dropped, but are also not compacted,
making them accessible only if referenced explicitly.  


## NodeJS example

NodeJS uses a `package.json` format that is very similar to a simple `codemeta.json` file, though it is not Linked Data as it does not declare a context.  Here we crosswalk an example `package.json` file into proper `codemeta` standard.  

```{r}
package.json <- read_json(
"https://raw.githubusercontent.com/heroku/node-js-sample/master/package.json")
package.json
```


```{r}
crosswalk(package.json, "NodeJS")
```

Note that while nested structures per se pose no special problem, the compaction/expansion paradigm lacks a mechanism to capture differences in nesting between schema.  For instance,
in `codemeta` (i.e. in schema.org), a `codeRepository` is expected to be a URL, while NodeJS `package.json` permits it to be another object node with sub-properties `type` and `url`.  There is no way in JSON-LD transforms or context definitions to indicate that the `url` sub-property in the NodeJS case, e.g. `codeRepository.url` maps to schema's `codeRepository`. 
(This same limitation is also true of the 2-dimensional table structure of the crosswalk itself, though it is important to keep in mind that this 1:1 mapping requirement is not unique to the the `.csv` representation but also inherent in JSON-LD contexts.)  

Consequently, a thorough translation between formats that do not provide their own JSON-LD contexts will ultimately require more manual implementation, which would be expressed within a particular programming language (e.g. R) rather than in the generic algorithms of JSON-LD available in many common programming languages.  




